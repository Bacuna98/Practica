{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Conversation with ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv('.env') # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py:200: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "c:\\Users\\benja\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py:785: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there! I am Sam\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there! I am Sam\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello, Sam! It's a pleasure to meet you. I'm an AI developed to assist with information and tasks. How can I help you today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(model_name='text-davinci-003', \n",
    "             temperature=0, \n",
    "             max_tokens = 256)\n",
    "\n",
    "llm2 = OpenAI(model_name='gpt-4', \n",
    "            temperature=0, \n",
    "            max_tokens = 256)\n",
    "     \n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory2 = ConversationBufferMemory()\n",
    "\n",
    "     \n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "conversation2 = ConversationChain(\n",
    "    llm=llm2, \n",
    "    verbose=True, \n",
    "    memory=memory2\n",
    ")\n",
    "     \n",
    "\n",
    "conversation.predict(input=\"Hi there! I am Sam\")\n",
    "conversation2.predict(input=\"Hi there! I am Sam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there! I am Sam\n",
      "AI:  Hi Sam! My name is AI. It's nice to meet you. What brings you here today?\n",
      "Human: How are you today?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there! I am Sam\n",
      "AI: Hello, Sam! It's a pleasure to meet you. I'm an AI developed to assist with information and tasks. How can I help you today?\n",
      "Human: How are you today?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"As an artificial intelligence, I don't have feelings or emotions, but I'm functioning as expected and ready to assist you. What can I do for you today, Sam?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"How are you today?\")\n",
    "conversation2.predict(input=\"How are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there! I am Sam\n",
      "AI:  Hi Sam! My name is AI. It's nice to meet you. What brings you here today?\n",
      "Human: How are you today?\n",
      "AI:  I'm doing great, thanks for asking! How about you?\n",
      "Human: I'm good thank you. Can you help me with some customer support?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there! I am Sam\n",
      "AI: Hello, Sam! It's a pleasure to meet you. I'm an AI developed to assist with information and tasks. How can I help you today?\n",
      "Human: How are you today?\n",
      "AI: As an artificial intelligence, I don't have feelings or emotions, but I'm functioning as expected and ready to assist you. What can I do for you today, Sam?\n",
      "Human: I'm good thank you. Can you help me with some customer support?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Of course, Sam! I'd be happy to assist you with customer support. Could you please provide me with more details about the issue you're facing or the information you need?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm good thank you. Can you help me with some customer support?\")\n",
    "conversation2.predict(input=\"I'm good thank you. Can you help me with some customer support?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi there! I am Sam\n",
      "AI:  Hi Sam! My name is AI. It's nice to meet you. What brings you here today?\n",
      "Human: How are you today?\n",
      "AI:  I'm doing great, thanks for asking! How about you?\n",
      "Human: I'm good thank you. Can you help me with some customer support?\n",
      "AI:  Absolutely! What kind of customer support do you need?\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi there! I am Sam\n",
      "AI: Hello, Sam! It's a pleasure to meet you. I'm an AI developed to assist with information and tasks. How can I help you today?\n",
      "Human: How are you today?\n",
      "AI: As an artificial intelligence, I don't have feelings or emotions, but I'm functioning as expected and ready to assist you. What can I do for you today, Sam?\n",
      "Human: I'm good thank you. Can you help me with some customer support?\n",
      "AI: Of course, Sam! I'd be happy to assist you with customer support. Could you please provide me with more details about the issue you're facing or the information you need?\n"
     ]
    }
   ],
   "source": [
    "print(conversation2.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métrica ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = []\n",
    "lista.append(conversation.memory.buffer)\n",
    "\n",
    "lista2 = []\n",
    "lista2.append(conversation2.memory.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.78125, recall=0.43103448275862066, fmeasure=0.5555555555555556), mid=Score(precision=0.78125, recall=0.43103448275862066, fmeasure=0.5555555555555556), high=Score(precision=0.78125, recall=0.43103448275862066, fmeasure=0.5555555555555556)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.5238095238095238, recall=0.28695652173913044, fmeasure=0.37078651685393255), mid=Score(precision=0.5238095238095238, recall=0.28695652173913044, fmeasure=0.37078651685393255), high=Score(precision=0.5238095238095238, recall=0.28695652173913044, fmeasure=0.37078651685393255)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.703125, recall=0.3879310344827586, fmeasure=0.5), mid=Score(precision=0.703125, recall=0.3879310344827586, fmeasure=0.5), high=Score(precision=0.703125, recall=0.3879310344827586, fmeasure=0.5)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.765625, recall=0.4224137931034483, fmeasure=0.5444444444444444), mid=Score(precision=0.765625, recall=0.4224137931034483, fmeasure=0.5444444444444444), high=Score(precision=0.765625, recall=0.4224137931034483, fmeasure=0.5444444444444444))}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "rouge = load_metric(\"rouge\")\n",
    "rouge.compute(predictions=lista, references=lista2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.compute(predictions=lista, references=lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métrica Bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Conversation with ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py:200: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "c:\\Users\\benja\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py:785: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "client = Client()\n",
    "\n",
    "llm = OpenAI(model_name='gpt-4', \n",
    "             temperature=0, \n",
    "             max_tokens = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting k=2, will only keep the last 2 interactions in memory\n",
    "# max_token_limit=40 - token limits needs transformers installed\n",
    "memory = ConversationSummaryBufferMemory(llm=OpenAI(), max_token_limit=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there! I am Sam\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello, Sam! It's a pleasure to meet you. I'm an AI developed to assist with information and tasks. How can I help you today?\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Hi there! I am Sam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human introduces themselves as Sam.\n",
      "AI: Hello, Sam! It's a pleasure to meet you. I'm an AI developed to assist with information and tasks. How can I help you today?\n",
      "Human: I need help with my broken TV\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry to hear about your broken TV, Sam. Could you please provide me with more details? For instance, what's the brand and model of your TV? What seems to be the problem? Is it not turning on, or is there an issue with the picture or sound? The more information you can provide, the better I can assist you.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"I need help with my broken TV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human introduces themselves as Sam and the AI greets them warmly. The AI has been developed to assist with information and tasks and it asks Sam how it can help. Sam mentions they need help with their broken TV, and the AI asks for more details to better assist them.\n",
      "Human: It makes weird sounds when i turn it on and then goes black\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry to hear about your TV issue, Sam. It sounds like it could be a problem with the internal components, possibly the power supply or the backlight. However, I'm an AI and I don't have the ability to physically inspect or repair your TV. I would recommend contacting a professional technician for a proper diagnosis. In the meantime, you could try unplugging the TV, waiting for a few minutes, and then plugging it back in to see if that resolves the issue.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"It makes weird sounds when i turn it on and then goes black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human introduces themselves as Sam and the AI greets them warmly. The AI has been developed to assist with information and tasks and it asks Sam how it can help. Sam mentions they need help with their broken TV, and the AI asks for more details to better assist them. After Sam explains the issue, the AI states that it doesn't have the ability to physically inspect or repair the TV, but it recommends contacting a professional technician for a proper diagnosis and suggests unplugging the TV for a few minutes and plugging it back in to see if that resolves the issue.\n",
      "Human: It seems to be Hardware\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I see, if it's a hardware issue, it could be a variety of things. It might be a problem with the power supply, the screen, or even the internal components. However, without a proper diagnosis, it's hard to say for sure. I would recommend reaching out to a professional technician who can inspect the TV and identify the exact issue. They have the necessary tools and expertise to safely handle and repair hardware problems.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"It seems to be Hardware\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The human introduces themselves as Sam and the AI greets them warmly. The AI has been developed to assist with information and tasks and it asks Sam how it can help. Sam mentions they need help with their broken TV, and the AI asks for more details to better assist them. After Sam explains the issue, the AI states that it doesn't have the ability to physically inspect or repair the TV, but it recommends contacting a professional technician for a proper diagnosis and suggests unplugging the TV for a few minutes and plugging it back in to see if that resolves the issue. Sam mentions the issue might be Hardware, to which the AI states that it could be a variety of things, such as a problem with the power supply, the screen, or even the internal components. However, without a proper diagnosis, it's hard to say for sure and recommends reaching out to a professional technician who can inspect the TV and identify the exact issue.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_with_summary.memory.moving_summary_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probando otra cosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getcontext\n",
    "from getcontext.generated.models import Conversation, Message, MessageRole, Rating\n",
    "from getcontext.token import Credential\n",
    "import os\n",
    "\n",
    "token = \"xSJtmgvXM9FX2pYoLu5cNij2\"\n",
    "c = getcontext.ContextAPI(credential=Credential(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "# List only the LLM runs in the project\n",
    "runs = client.list_runs(\n",
    "    project_name=os.environ[\"LANGCHAIN_PROJECT\"],\n",
    "    run_type=\"llm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'messages'\n",
      "'messages'\n",
      "'messages'\n",
      "'messages'\n",
      "'messages'\n",
      "'messages'\n",
      "'messages'\n",
      "'messages'\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.load import load\n",
    "import json\n",
    "\n",
    "\n",
    "def _get_role(m):\n",
    "    if isinstance(m, HumanMessage):\n",
    "        return MessageRole.USER\n",
    "    elif isinstance(m, AIMessage):\n",
    "        return MessageRole.ASSISTANT\n",
    "    elif isinstance(m, SystemMessage):\n",
    "        return MessageRole.SYSTEM\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "for run in runs:\n",
    "    # This only works for chat messages\n",
    "    try:\n",
    "        messages = [load.loads(json.dumps(m)) for m in run.inputs[\"messages\"]]\n",
    "\n",
    "        messages.append(\n",
    "            load.loads(json.dumps(run.outputs[\"generations\"][0][\"message\"]))\n",
    "        )\n",
    "        context_messages = [\n",
    "            Message(message=m.content, role=_get_role(m)) for m in messages\n",
    "        ]\n",
    "        c.log.conversation(\n",
    "            body={\n",
    "                \"conversation\": Conversation(\n",
    "                    messages=context_messages,\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
