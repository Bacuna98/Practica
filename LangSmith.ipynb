{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "078b8d95-bde8-4312-b3d0-41de7c3b1b8a",
   "metadata": {},
   "source": [
    "# Evaluación de sistemas de preguntas y respuestas con datos dinámicos\n",
    "En muchos escenarios del mundo real, la respuesta correcta a una pregunta puede cambiar con el tiempo. Por ejemplo, si está creando un sistema de preguntas y respuestas sobre una base de datos o que se conecta a una API, los datos subyacentes pueden actualizarse con frecuencia. En tales casos, aún querrá medir la corrección de su sistema, pero querrá hacerlo de una manera que tenga en cuenta estos cambios.\n",
    "\n",
    "En el siguiente tutorial, utilizaremos la antigua práctica del software de direccionamiento indirecto para solucionar este problema. En lugar de almacenar etiquetas directamente como valores, nuestras etiquetas serán referencias para buscar los valores correctos. En este caso, nuestras etiquetas serán consultas que el evaluador personalizado puede usar para obtener la respuesta de verdad fundamental y compararla con las predicciones del modelo.\n",
    "\n",
    "El tutorial lo guiará a través de los siguientes pasos:\n",
    "\n",
    "Cree un conjunto de datos de preguntas y fragmentos de código correspondientes para obtener las respuestas.\n",
    "Defina su sistema de preguntas y respuestas.\n",
    "Ejecute la evaluación utilizando LangSmith con un evaluador personalizado.\n",
    "Vuelva a probar el sistema con el tiempo.\n",
    "Nota rápida: estamos utilizando un archivo CSV para simular una fuente de datos real. Este no es un escenario real y pretende ser ilustrativo.\n",
    "\n",
    "¡Vamos a empezar!\n",
    "\n",
    "## Prerequisitos\n",
    "\n",
    "Este tutorial usa OpenAI para el modelo y LangChain para componer la cadena. Para asegurarse de que el seguimiento y las evaluaciones estén configurados para LangSmith, configure su clave API de manera adecuada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655920e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv() # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e258e15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.predict(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b810a9e",
   "metadata": {},
   "source": [
    "### 1. Create a dataset\n",
    "\n",
    "Usaremos el conjunto de datos Titanic de <https://raw.githubusercontent.com/jorisvandenbossche/pandas-tutorial/master/data/titanic.csv> para nuestro ejemplo. Este conjunto de datos contiene información sobre los pasajeros del Titanic, junto con sus resultados.\n",
    "\n",
    "Primero, defina un conjunto de preguntas y referencias correspondientes que muestren cómo recuperar la respuesta correcta de los datos. Por el bien del tutorial, usaremos fragmentos de código de Python, pero la táctica se puede aplicar generalmente a cualquier otra forma de direccionamiento indirecto, como almacenar solicitudes de API o argumentos de búsqueda.\n",
    "\n",
    "Nuestro evaluador utilizará las referencias para obtener la respuesta correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29fa8c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    (\"How many passengers were on the Titanic?\", \"len(df)\"),\n",
    "    (\"How many passengers survived?\", \"df['Survived'].sum()\"),\n",
    "    (\"What was the average age of the passengers?\", \"df['Age'].mean()\"),\n",
    "    (\"How many male and female passengers were there?\", \"df['Sex'].value_counts()\"),\n",
    "    (\"What was the average fare paid for the tickets?\", \"df['Fare'].mean()\"),\n",
    "    (\"How many passengers were in each class?\", \"df['Pclass'].value_counts()\"),\n",
    "    (\"What was the survival rate for each gender?\", \"df.groupby('Sex')['Survived'].mean()\"),\n",
    "    (\"What was the survival rate for each class?\", \"df.groupby('Pclass')['Survived'].mean()\"),\n",
    "    (\"Which port had the most passengers embark from?\", \"df['Embarked'].value_counts().idxmax()\"),\n",
    "    (\"How many children under the age of 18 survived?\", \"df[df['Age'] < 18]['Survived'].sum()\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad5f611",
   "metadata": {},
   "source": [
    "\n",
    "​A continuación, ¡cree el conjunto de datos! Puede utilizar el SDK de LangSmith para hacerlo. Cree el conjunto de datos y cargue cada ejemplo. Guardar el conjunto de datos en LangSmith nos permite reutilizar y relacionar las ejecuciones de prueba a lo largo del tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f65f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Dynamic Titanic CSV3135\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name, description=\"Test QA over CSV\",\n",
    ")\n",
    "\n",
    "for example in questions:\n",
    "    client.create_example(\n",
    "        inputs={\"question\": example[0]},\n",
    "        outputs={\"code\": example[1]},\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda0013a",
   "metadata": {},
   "source": [
    "### 2. Definir el sistema de preguntas y respuestas\n",
    "Con el conjunto de datos creado, es hora de definir nuestro sistema de respuesta a preguntas. Usaremos el agente de marco de datos pandas comercial de LangChain para este tutorial.\n",
    "\n",
    "Primero, cargue los datos dinámicos en un marco de datos, luego cree un constructor para nuestro agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87eff2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "titanic_path = \"https://raw.githubusercontent.com/jorisvandenbossche/pandas-tutorial/master/data/titanic.csv\"\n",
    "df = pd.read_csv(titanic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cabe7504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.agents import create_pandas_dataframe_agent\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.0)\n",
    "create_chain = partial(\n",
    "    create_pandas_dataframe_agent,\n",
    "    llm=llm,\n",
    "    df=df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4efcc2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How many passengers were on the Titanic?',\n",
       " 'output': 'There were 891 passengers on the Titanic.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example run\n",
    "create_chain().invoke({\"input\": \"How many passengers were on the Titanic?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e47885",
   "metadata": {},
   "source": [
    "### 3. Ejecutar evaluación\n",
    "Ahora es el momento de definir nuestro evaluador personalizado. En este caso heredaremos de la clase LabeledCriteriaEvalChain. Este evaluador toma la entrada, la predicción y la etiqueta de referencia y los pasa a un LLM para predecir si la predicción satisface los criterios proporcionados, condicionados a la etiqueta de referencia.\n",
    "\n",
    "Nuestro evaluador personalizado hará un pequeño cambio en este evaluador eliminando la referencia de la etiqueta para inyectar el valor correcto. Hacemos esto sobrescribiendo el método _get_eval_input. Luego, el LLM verá el nuevo valor de referencia.\n",
    "\n",
    "Recordatorio: estamos usando un archivo CSV para simular una fuente de datos real aquí y estamos haciendo una evaluación insegura para consultar la fuente de datos. En un escenario real, sería mejor hacer una solicitud de obtención segura o algo similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17f6772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "from typing import Optional\n",
    "from langchain.evaluation.criteria.eval_chain import LabeledCriteriaEvalChain\n",
    "\n",
    "class CustomCriteriaEvalChain(LabeledCriteriaEvalChain):\n",
    "    def _get_eval_input(\n",
    "        self,\n",
    "        prediction: str,\n",
    "        reference: Optional[str],\n",
    "        input: Optional[str],\n",
    "    ) -> dict:\n",
    "        # The parent class validates the reference is present and combines into\n",
    "        # a dictionary for the llm chain.\n",
    "        raw = super()._get_eval_input(prediction, reference, input)\n",
    "        # Warning - this evaluates the code you've saved as labels in the dataset.\n",
    "        # Be sure that the code is correct, and refrain from executing in an\n",
    "        # untrusted environment or when connected to a production server.\n",
    "        raw[\"reference\"] = eval(raw[\"reference\"])\n",
    "        return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4901844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'e86f34530bc94e3b88d4e312c98cc01f-AgentExecutor' at:\n",
      "https://smith.langchain.com/projects/p/3a68edb6-7748-4612-b2e5-d81d7db92d0b?eval=true\n"
     ]
    }
   ],
   "source": [
    "client = Client()\n",
    "eval_config = RunEvalConfig(\n",
    "    custom_evaluators=[\n",
    "        CustomCriteriaEvalChain.from_llm(criteria=\"correctness\", llm=ChatOpenAI(model=\"gpt-4\", temperature=0.0)),\n",
    "    ],\n",
    ")\n",
    "chain_results = run_on_dataset(\n",
    "    client,\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=create_chain,\n",
    "    evaluation=eval_config,\n",
    "    # This agent doesn't support concurrent runs yet.\n",
    "    concurrency_level=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5222b69e",
   "metadata": {},
   "source": [
    "Con esa evaluación en ejecución, puede navegar hasta el proyecto vinculado y revisar las predicciones y los puntajes de retroalimentación del agente.\n",
    "\n",
    "### 4. Vuelva a evaluar más tarde.\n",
    "Es seguro decir que el conjunto de datos del Titanic no ha cambiado en los últimos minutos, pero en su caso, es probable que ingresen nuevos datos todo el tiempo. Mientras la forma de acceder a esa información no haya cambiado, ¡podemos reutilizar el conjunto de datos existente!\n",
    "\n",
    "Supongamos que más personas abordaron duplicando algunas filas y barajando algunas estadísticas. Luego, volveremos a ejecutar el agente en el nuevo conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f62d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doubled = pd.concat([df, df], ignore_index=True)\n",
    "df_doubled['Age'] = df_doubled['Age'].sample(frac=1).reset_index(drop=True)\n",
    "df_doubled['Sex'] = df_doubled['Sex'].sample(frac=1).reset_index(drop=True)\n",
    "df = df_doubled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae9493af",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_chain_2 = partial(\n",
    "    create_pandas_dataframe_agent,\n",
    "    llm=llm,\n",
    "    df=df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "597845d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '31fc6dfb40534f038071091e09db44c7-AgentExecutor' at:\n",
      "https://smith.langchain.com/projects/p/58aece77-8ed2-4ef9-96ec-d3015af0cd45?eval=true\n"
     ]
    }
   ],
   "source": [
    "chain_results = run_on_dataset(\n",
    "    client,\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=create_chain_2,\n",
    "    evaluation=eval_config,\n",
    "    concurrency_level=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3646a26b",
   "metadata": {},
   "source": [
    "#### Revisa los resultados\n",
    "Ahora que ha probado dos veces en la fuente de datos \"cambiante\", ¡puede ver los resultados! Si navega a la página \"conjunto de datos\" y hace clic en la pestaña \"ejemplos\", puede hacer clic en diferentes ejemplos y ver las predicciones para cada ejecución de prueba.\n",
    "\n",
    "A continuación se muestra la vista de las filas de conjuntos de datos individuales. Podemos hacer clic en una fila para actualizar el ejemplo o para ver todas las predicciones de diferentes ejecuciones de prueba en ese ejemplo. ¡Hagamos clic en uno!\n",
    "\n",
    "En este caso, hemos seleccionado la fila de ejemplo con la pregunta \"¿Cuántos pasajeros hombres y mujeres había?\" La tabla de filas vinculadas en la parte inferior de la página muestra las predicciones para cada ejecución de prueba. Estos se asocian automáticamente cada vez que llama a run_on_dataset.\n",
    "\n",
    "¡Si miras de cerca las predicciones, verás que las predicciones son diferentes! En un principio, el agente pronosticó 577 pasajeros masculinos y 314 femeninos. Luego, para la segunda prueba, predijo 1154 pasajeros masculinos y 628 femeninos.\n",
    "\n",
    "Sin embargo, ambas ejecuciones de prueba se marcaron como \"correctas\". Los valores dentro de la fuente de datos cambiaron, pero el proceso para recuperar la respuesta siguió siendo el mismo.\n",
    "\n",
    "Pero, ¿cómo puede estar seguro de que la calificación \"correcta\" es confiable? Ahora es un buen momento para verificar el seguimiento de ejecución de su evaluador personalizado para confirmar que funciona como se esperaba. Si ve flechas en las fichas de \"corrección\" en la tabla, puede hacer clic directamente en ellas para ver el seguimiento de la evaluación. De lo contrario, puede hacer clic en la ejecución, navegar a la pestaña de comentarios y luego hacer clic para encontrar el seguimiento de su evaluador personalizado para ese ejemplo. A continuación se muestran capturas de pantalla de los valores recuperados para cada una de las ejecuciones anteriores.\n",
    "\n",
    "Puede ver que la clave de \"referencia\" contiene el valor desreferenciado de la fuente de datos. ¡Puedes ver que coincide con las predicciones de las carreras anteriores! El primero muestra 577 pasajeros masculinos y 314 femeninos.\n",
    "\n",
    "Y después de que se actualizó el marco de datos, el evaluador recuperó el valor correcto de 1154 pasajeros masculinos y 628 femeninos, ¡lo que coincide con las predicciones de las carreras anteriores!\n",
    "\n",
    "Página de ejemplos\n",
    "¡Parece estar funcionando bien!\n",
    "\n",
    "#### Conclusiones\n",
    "\n",
    "En este tutorial, evaluó un sistema de preguntas y respuestas conectado a un almacén de datos en evolución. Lo hizo usando un evaluador personalizado que obtiene dinámicamente la respuesta en función de una referencia estática (en este caso, un fragmento de código).\n",
    "\n",
    "¡Esta es solo una forma de abordar el problema de evaluar los sistemas de preguntas y respuestas cuando la fuente de datos subyacente está cambiando! Este enfoque es simple y prueba directamente la corrección de su sistema de principio a fin con datos actualizados. Puede ser útil si desea controlar su desempeño periódicamente.\n",
    "\n",
    "Es menos confiable si su objetivo es comparar dos indicaciones o modelos diferentes, ya que los datos subyacentes pueden diferir. Dependiendo de cómo elimine las referencias de las etiquetas, ¡la precaución y los permisos adecuados también son importantes!\n",
    "\n",
    "Otras opciones para evaluar su sistema en este escenario incluyen:\n",
    "\n",
    "Congelar o burlarse de las fuentes de datos utilizadas para la evaluación. Luego puede invertir en el etiquetado manual periódicamente para asegurarse de que los datos sigan siendo representativos del entorno de producción.\n",
    "Probar la capacidad de generación de consultas de su agente directamente y evaluar la equivalencia de las consultas. Esto es menos \"extremo a extremo\", pero dependiendo de cómo se compare, evitará cualquier problema potencial causado por la desreferenciación insegura.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2ba3e",
   "metadata": {},
   "source": [
    "# Revisar el estado del arte de las metricas de las aplicaciones de LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4fcbf4",
   "metadata": {},
   "source": [
    "En la última década, los avances en el campo de la Inteligencia Artificial (IA) han llevado a la creación de modelos de lenguaje cada vez más grandes y sofisticados. Estos modelos, conocidos como Modelos de Lenguaje Largo (LLM), han demostrado su eficacia en una amplia gama de aplicaciones, como generación de texto, traducción automática, resumen de texto y respuesta a preguntas. La evaluación precisa y objetiva de estos modelos es esencial para medir su rendimiento y garantizar resultados de alta calidad en diversas tareas.\n",
    "\n",
    "\n",
    "## Perplejidad\n",
    "Es una métrica de evaluación de modelado lingüístico intrínseca que mide la inversa de la probabilidad de media geométrica por palabra en los datos de prueba. Una puntuación de perplejidad más baja indica un mejor rendimiento de generalización. Las investigaciones han demostrado que la probabilidad calculada por palabra a menudo no se alinea con el juicio humano y puede no estar totalmente correlacionada, por lo que se ha introducido la coherencia temática. \n",
    "\n",
    "Se puede encontrar que una frase $x_i$ en el conjunto de prueba puede ser codificada de media mediante $190$ bits (p.e, si las frases de prueba tienen una media de probabilidad logarítmica de $-190$). Este valor nos daría un valor enorme de perplejidad de $2190$ por frase. Sin embargo, es más común normalizar por la longitud de la frase y considerar sólo el número de bits por palabra. De esta manera, si las frases de un conjunto de entrenamiento constan de un total de $1000$ palabras, y éstas se codifican mediante $7950$ bits, se da la perplejidad del modelo como $2^{7.95} = 247$ por palabra. En otras palabras, el modelo se confunde en el conjunto de prueba como si tuviese que elegir uniforme e independientemente entre 247 posibilidades por cada palabra.\n",
    "\n",
    "## BLEU (Bilingual Evaluation Understudy)\n",
    "Es un método de evaluación de la calidad de traducciones realizadas por sistemas de traducción automática. Una traducción tiene mayor calidad cuanto más similar es con respecto a otra de referencia, que se supone correcta. BLEU puede calcularse utilizando más de una traducción de referencia. Esto permite una mayor robustez a la medida frente a traducciones libres realizadas por humanos.\n",
    "BLEU se calcula normalmente a nivel de frases y halla la precisión en ngramas entre la traducción del sistema y la de referencia. Sin embargo, se utiliza una precisión modificada con el fin de solucionar ciertas deficiencias en la medida.\n",
    "\n",
    "\n",
    "## ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "Es un conjunto de métricas y un paquete de software que se utiliza para evaluar el software de resumen automático y traducción automática en el procesamiento del lenguaje natural. Las métricas comparan un resumen o traducción producido automáticamente con una referencia o un conjunto de resumen o traducción de referencias (producidas por humanos). Explicaremos las tres más comunes\n",
    "1. La primera de ellas es ROUGE-1. Esta variante de la métrica calcula la precisión y el recall como hemos explicado previamente, a partir de los unigramas del resumen generado y del resumen esperado.\n",
    "2. La segunda de ellas es ROUGE-2. Esta variante es similar a ROUGE-1, pero usa bigramas en vez de unigramas.\n",
    "3. La tercera de ellas es ROUGE-L. Esta variante es bastante diferente de las anteriores. Lo que hace es buscar la cadena común mas larga contenida dentro de ambos resúmenes.\n",
    "\n",
    "Existen más tipos de métricas, y la ROUGE-1 y ROUGE-2 se pueden agrupar en la denominada ROUGE-N, que incluye todos los tipos de n-gramas.\n",
    "\n",
    "## METEOR (Métrica para la evaluación de la traducción con ordenación explícita)\n",
    "Es una métrica para la evaluación de los resultados de la traducción automática. La métrica se basa en la media armónica de la precisión y la recuperación de unigramas, con la recuperación ponderada más que la precisión. También tiene varias características que no se encuentran en otras métricas, como la derivación y la coincidencia de sinónimos, junto con la coincidencia estándar de palabras exactas. La métrica fue diseñada para solucionar algunos de los problemas encontrados en la métrica BLEU más popular y también producir una buena correlación con el juicio humano a nivel de oración o segmento. Esto se diferencia de la métrica BLEU en que BLEU busca correlación a nivel de corpus.\n",
    "\n",
    "## SARI (System-level Automatic Review Interface)\n",
    "Se utiliza para evaluar la calidad de las reformulaciones generadas por el modelo, especialmente en tareas de resumen y paráfrasis.\n",
    "\n",
    "## F1-score\n",
    "A menudo utilizado para medir la precisión y exhaustividad en tareas de extracción de información o respuesta a preguntas.\n",
    "\n",
    "## Extras\n",
    "\n",
    "### 1.- Similaridad de Coseno: Métrica de Evaluación\n",
    "\n",
    "La **Similaridad de Coseno** es una métrica comúnmente utilizada para medir la similitud entre vectores de características. Aunque no es específica de los modelos de lenguaje largo, es ampliamente utilizada en varias aplicaciones que involucran representaciones vectoriales, como modelos de lenguaje y recuperación de información.\n",
    "\n",
    "La Similaridad de Coseno mide la orientación y la similitud direccional entre dos vectores en un espacio n-dimensional. Se puede utilizar para medir la similitud semántica entre dos frases, oraciones o documentos representados como vectores numéricos.\n",
    "\n",
    "La fórmula de la Similaridad de Coseno es:\n",
    "\n",
    "$$ \\text{Similadirad de Coseno} = \\frac{A \\cdot B}{||A|| \\cdot ||B||}$$\n",
    "\n",
    "Donde A y B son los vectores que se están comparando, y ||A|| y ||B|| son las magnitudes de los vectores (longitudes euclidianas). El resultado está en el rango de -1 (vectores opuestos) a 1 (vectores idénticos), y valores cercanos a 0 indican similitud baja.\n",
    "\n",
    "La Similaridad de Coseno se utiliza en varias aplicaciones, como:\n",
    "\n",
    "- **Recuperación de Información:** En motores de búsqueda, esta métrica se usa para calcular la similitud entre el término de búsqueda y los documentos relevantes.\n",
    "\n",
    "- **Clasificación de Texto:** Para medir la similitud entre documentos en tareas de agrupamiento o categorización.\n",
    "\n",
    "- **Modelos de Lenguaje y Word Embeddings:** En modelos de lenguaje como Word2Vec y GloVe, la similitud de coseno se utiliza para encontrar palabras o frases similares en el espacio vectorial.\n",
    "\n",
    "- **Filtrado Colaborativo:** En sistemas de recomendación, se utiliza para medir la similitud entre perfiles de usuario y elementos recomendados.\n",
    "\n",
    "- **Procesamiento de Lenguaje Natural:** Para medir la similitud semántica entre oraciones, preguntas y respuestas, lo que es relevante en tareas como el análisis de sentimientos y la generación de respuestas automáticas.\n",
    "\n",
    "La Similaridad de Coseno no es una métrica específica de rendimiento, como BLEU o ROUGE, sino más bien una herramienta para medir la similitud entre vectores. Puede ser especialmente útil en aplicaciones donde se busca determinar qué tan cercanos están los vectores de características, lo que es fundamental en tareas de procesamiento de lenguaje natural y recuperación de información.\n",
    "\n",
    "### 2.- Jaccard Similarity:\n",
    "La Similaridad de Jaccard se utiliza para medir la similitud entre conjuntos. En lugar de considerar los valores numéricos en los vectores, Jaccard se enfoca en la presencia o ausencia de elementos en los conjuntos. Es útil para medir la similitud entre conjuntos de elementos, como palabras clave o categorías.\n",
    "\n",
    "### 3.- Pearson Correlation Coefficient:\n",
    "El Coeficiente de Correlación de Pearson mide la relación lineal entre dos conjuntos de datos. A menudo se utiliza para evaluar la correlación entre variables, y puede adaptarse para medir la similitud entre vectores. Sin embargo, esta métrica es más adecuada cuando se busca una relación lineal específica.\n",
    "\n",
    "### 4.- Coseno Adaptable:\n",
    "Esta métrica es una variante de la Similaridad de Coseno que tiene en cuenta la magnitud de los vectores, lo que puede ser útil cuando se desea penalizar más a los vectores largos. En lugar de simplemente dividir el producto interno por el producto de las magnitudes, se divide por la suma ponderada de las magnitudes.\n",
    "\n",
    "### 5.- TF-IDF (Term Frequency-Inverse Document Frequency):\n",
    "Aunque no es una métrica de similitud directa, TF-IDF es un enfoque que pondera la importancia relativa de las palabras en un documento o conjunto de documentos. Se utiliza comúnmente en la recuperación de información y la minería de texto para identificar palabras clave y calcular la relevancia.\n",
    "\n",
    "### 6- Word Embeddings y Distancias de Palabras:\n",
    "En el contexto del procesamiento de lenguaje natural, los word embeddings como Word2Vec y FastText generan vectores semánticos para palabras. Luego, se pueden utilizar medidas de distancia (como la distancia coseno) para comparar la similitud semántica entre palabras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#![image.png](attachment:image.png)\n",
    "#![image-2.png](attachment:image-2.png)\n",
    "#![image-3.png](attachment:image-3.png)\n",
    "#![image-4.png](attachment:image-4.png)\n",
    "#![image-5.png](attachment:image-5.png)\n",
    "#![image-6.png](attachment:image-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b7ccc",
   "metadata": {},
   "source": [
    "# Como hacer un vínculo entre ambas cosas, langchain y metricas en app usando llm\n",
    "\n",
    "Al aprovechar la diversidad de proveedores LLM que ofrece \"LangChain\", los usuarios pueden aplicar una variedad de métricas de evaluación para medir la calidad y eficacia de las salidas generadas. Una de estas métricas es la **Similaridad de Coseno**, que mide la similitud semántica entre vectores de características. Al integrar esta métrica en la funcionalidad de \"LangChain\", los usuarios pueden evaluar la coherencia y la precisión de las respuestas generadas por los LLMs conectados en la cadena.\n",
    "\n",
    "Además, en el contexto de traducción automática proporcionada por los proveedores LLM en \"LangChain\", las métricas como **BLEU** y **ROUGE** podrían aplicarse para evaluar la calidad de las traducciones generadas. Esto garantizaría que las cadenas de modelos de lenguaje largo en \"LangChain\" produzcan traducciones precisas y coherentes, proporcionando resultados más satisfactorios para los usuarios.\n",
    "\n",
    "### Similaridad de Coseno\n",
    "\n",
    "En el contexto de LangChain, esta métrica podría ser utilizada para evaluar la similitud semántica entre las respuestas generadas por los distintos modelos de lenguaje largo conectados en una cadena. Por ejemplo, si LangChain combina respuestas de diferentes proveedores LLM, la Similaridad de Coseno podría ayudar a identificar si las respuestas son coherentes entre sí. Si la similitud de coseno entre dos respuestas es alta, esto podría indicar que las respuestas son consistentes y concuerdan en su contenido.\n",
    "\n",
    "### BLEU (Bilingual Evaluation Understudy) y ROUGE (Recall-Oriented Understudy for Gisting Evaluation):\n",
    "Estas métricas son comúnmente utilizadas para evaluar la calidad de traducciones automáticas o resúmenes generados por modelos de lenguaje largo. En el contexto de LangChain, donde varios proveedores LLM se utilizan para la generación de texto, BLEU y ROUGE podrían ser aplicados para evaluar la calidad de las traducciones o resúmenes generados por la cadena de modelos. Esto ayudaría a identificar cuál de los proveedores LLM está produciendo resultados más precisos y coherentes en términos de traducción y resumen.\n",
    "\n",
    "#### Ejemplo implementación Similaridad de Coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b84226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similaridad de Coseno entre las oraciones: 0.6299407883487119\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Ejemplo de oraciones\n",
    "sentence_1 = \"La lluvia en Sevilla es una maravilla.\"\n",
    "sentence_2 = \"En Sevilla, la lluvia es un fenómeno poco común.\"\n",
    "\n",
    "# Crear una matriz de frecuencia de términos (bag of words) para las oraciones\n",
    "vectorizer = CountVectorizer().fit_transform([sentence_1, sentence_2])\n",
    "\n",
    "# Obtener los vectores de las oraciones\n",
    "vector_sentence_1 = vectorizer.toarray()[0]\n",
    "vector_sentence_2 = vectorizer.toarray()[1]\n",
    "\n",
    "# Calcular la similaridad de coseno entre los vectores de oraciones\n",
    "similarity_score = cosine_similarity([vector_sentence_1], [vector_sentence_2])[0][0]\n",
    "print(f\"Similaridad de Coseno entre las oraciones: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac11a172",
   "metadata": {},
   "source": [
    "# Funciones que nos otorga LangSmith de forma resumida\n",
    "\n",
    "\n",
    "A continuación, se presenta una lista de todas las funciones definidas en los archivos client.py y run_helpers.py del repositorio de LangSmith SDK de Python, junto con una breve descripción de cada una:\n",
    "\n",
    "## client.py:\n",
    "* LangsmithClient(): Crea una instancia de cliente de LangSmith para interactuar con la plataforma.\n",
    "\n",
    "\n",
    "### Ejecuciones\n",
    "\n",
    "* upload_dataframe(df, name, input_keys, output_keys, description=None, data_type=ls_schemas.DataType.kv): Carga un DataFrame de pandas como ejemplos individuales en la plataforma LangSmith.\n",
    "* upload_csv(csv_file, *, name=None, description=None, input_keys=None, output_keys=None, parameters=None, data_type=None): Carga un archivo CSV como ejemplos individuales en la plataforma LangSmith.\n",
    "* create_run(agent_id, *, name=None, description=None, input_keys=None, output_keys=None, parameters=None, parent_run_id=None): Crea una nueva ejecución de agente para un agente específico, especificado por el ID del agente.\n",
    "* update_run(run_id, *, name=None, description=None, input_keys=None, output_keys=None, parameters=None, parent_run_id=None): Actualiza una ejecución de agente específica, especificada por el ID de la ejecución del agente.\n",
    "* _load_child_runs(run_id): Carga todas las ejecuciones de agente secundarias para una ejecución de agente específica, especificada por el ID de la ejecución del agente.\n",
    "* read_run(run_id): Esta función se utiliza para obtener detalles sobre una ejecución de agente específica, especificada por el ID de la ejecución del agente.\n",
    "* list_runs(): Obtiene una lista de todas las ejecuciones de agente disponibles en la plataforma LangSmith.\n",
    "read_run(run_id): Obtiene detalles sobre una ejecución de agente específica, especificada por el ID de la ejecución del agente.\n",
    "* share_run(run_id): Crea un enlace compartido para una ejecución de agente específica, especificada por el ID de la ejecución del agente.\n",
    "* unshare_run(run_id): Elimina un enlace compartido para una ejecución de agente específica, especificada por el ID de la ejecución del agente.\n",
    "* read_run_shared_link(run_id): Obtiene un enlace compartido para una ejecución de agente específica, especificada por el ID de la ejecución del agente.\n",
    "* run_is_shared(run_id): Esta función se utiliza para determinar si una ejecución de agente específica, especificada por el ID de la ejecución del agente, está compartida o no.\n",
    "\n",
    "### Project\n",
    "\n",
    "* create_project(name, *, description=None): Crea un nuevo proyecto en la plataforma LangSmith.\n",
    "* read_project(project_id): Obtiene detalles sobre un proyecto específico, especificado por el ID del proyecto.\n",
    "* list_projects(): Obtiene una lista de todos los proyectos disponibles en la plataforma LangSmith.\n",
    "* delete_project(project_id): Elimina un proyecto específico de la plataforma LangSmith.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "* create_dataset(name, *, description=None, input_keys=None, output_keys=None, parameters=None, data_type=None): Crea un nuevo conjunto de datos en la plataforma LangSmith.\n",
    "* read_dataset(dataset_id): Obtiene detalles sobre un conjunto de datos específico, especificado por el ID del conjunto de datos.\n",
    "* list_datasets(): Obtiene una lista de todos los conjuntos de datos disponibles en la plataforma LangSmith.\n",
    "* delete_dataset(dataset_id): Elimina un conjunto de datos específico de la plataforma LangSmith.\n",
    "\n",
    "\n",
    "* _get_data_type(dataset_id): Esta función se utiliza para obtener el tipo de datos de un conjunto de datos específico, especificado por el ID del conjunto de datos.\n",
    "\n",
    "\n",
    "### Example\n",
    "* create_llm_example(agent_id, *, name=None, description=None, input_values=None, output_values=None, parameters=None, parent_run_id=None): Crea un nuevo ejemplo de LLM para un agente específico, especificado por el ID del agente.\n",
    "* create_chat_example(agent_id, *, name=None, description=None, input_values=None, output_values=None, parameters=None, parent_run_id=None): Crea un nuevo ejemplo de chat para un agente específico, especificado por el ID del agente.\n",
    "* create_example_from_run(run_id, *, name=None, description=None, input_values=None, output_values=None, parameters=None, parent_run_id=None): Crea un nuevo ejemplo a partir de una ejecución de agente específica, especificada por el ID de la ejecución del agente.\n",
    "* create_example(dataset_id, *, name=None, description=None, input_values=None, output_values=None, parameters=None, parent_run_id=None): Crea un nuevo ejemplo para un conjunto de datos específico, especificado por el ID del conjunto de datos.\n",
    "* read_example(example_id): Obtiene detalles sobre un ejemplo específico, especificado por el ID del ejemplo.\n",
    "* list_examples(): Obtiene una lista de todos los ejemplos disponibles en la plataforma LangSmith.\n",
    "* update_example(example_id, *, name=None, description=None, input_keys=None, output_keys=None, parameters=None, parent_run_id=None): Actualiza un ejemplo específico, especificado por el ID del ejemplo.\n",
    "* delete_example(example_id): Elimina un ejemplo específico de la plataforma LangSmith.\n",
    "\n",
    "\n",
    "### Run\n",
    "* _resolve_run_id(run_id): Resuelve el ID de ejecución de agente especificado en una cadena de ejecución de agente completa.\n",
    "* _resolve_example_id(example_id): Resuelve el ID de ejemplo especificado en una cadena de ejemplo completa.\n",
    "\n",
    "* evaluate_run(run_id): Evalúa una ejecución de agente específica, especificada por el ID de la ejecución del agente.\n",
    "\n",
    "* aevaluate_run(run, evaluator, *, source_info=None, reference_example=None, load_child_runs=False): Esta función se utiliza para evaluar una ejecución de agente específica de manera asíncrona, especificada por el ID de la ejecución del agente. Se utiliza un evaluador específico para realizar la evaluación, y se pueden proporcionar información adicional sobre la fuente de la evaluación y un ejemplo de referencia para la evaluación.\n",
    "\n",
    "\n",
    "### Feedback\n",
    "* create_feedback(agent_run_id, *, name=None, description=None, input_keys=None, output_keys=None, parameters=None, parent_run_id=None): Crea un nuevo comentario para una ejecución de agente específica, especificada por el ID de la ejecución del agente.\n",
    "* update_feedback(feedback_id, *, name=None, description=None, input_keys=None, output_keys=None, parameters=None, parent_run_id=None): Actualiza un comentario específico, especificado por el ID del comentario.\n",
    "* read_feedback(feedback_id): Obtiene detalles sobre un comentario específico, especificado por el ID del comentario.\n",
    "* list_feedback(): Obtiene una lista de todos los comentarios disponibles en la plataforma LangSmith.\n",
    "* delete_feedback(feedback_id): Elimina un comentario específico de la plataforma LangSmith.\n",
    "\n",
    "* arun_on_dataset(dataset_id, agent_id, *, name=None, description=None, input_keys=None, output_keys=None, parameters=None, parent_run_id=None, feedback=None, feedback_type=None, feedback_name=None, feedback_description=None, feedback_input_keys=None, feedback_output_keys=None, feedback_parameters=None, feedback_parent_run_id=None): Esta función se utiliza para ejecutar un agente en un conjunto de datos específico, especificado por el ID del conjunto de datos. Se pueden proporcionar varios parámetros adicionales, como el nombre, la descripción, las claves de entrada y salida, los parámetros y el ID de ejecución de agente principal. También se pueden proporcionar comentarios y metadatos de retroalimentación para la ejecución del agente.\n",
    "\n",
    "* run_on_dataset(dataset_id, agent_id, *, name=None, description=None, input_keys=None, output_keys=None, parameters=None, parent_run_id=None, feedback=None, feedback_type=None, feedback_name=None, feedback_description=None, feedback_input_keys=None, feedback_output_keys=None, feedback_parameters=None, feedback_parent_run_id=None): Ejecuta un agente en un conjunto de datos específico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2b10ce",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
